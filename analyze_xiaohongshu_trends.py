#!/usr/bin/env python3
"""
å°çº¢ä¹¦çˆ†æ¬¾æ–‡æ¡ˆåˆ†æè„šæœ¬
æœç´¢è¿‘15å¤©çš„çˆ†æ¬¾æ–‡æ¡ˆå¹¶æ€»ç»“å…³é”®è¯
"""

import requests
import json
import re
import time
from datetime import datetime, timedelta
from collections import Counter
import jieba
import jieba.analyse

class XiaohongshuAnalyzer:
    def __init__(self, server_url="http://localhost:18060/mcp"):
        """åˆå§‹åŒ–MCPå®¢æˆ·ç«¯"""
        self.server_url = server_url
        self.session_id = None
        self.headers = {}
        
    def create_session(self):
        """åˆ›å»ºMCPä¼šè¯"""
        print("ğŸ”§ åˆ›å»ºMCPä¼šè¯...")
        
        init_data = {
            "jsonrpc": "2.0",
            "method": "initialize",
            "params": {
                "protocolVersion": "2025-06-18",
                "capabilities": {
                    "tools": {},
                    "logging": {},
                    "resources": {}
                },
                "clientInfo": {
                    "name": "xiaohongshu-trend-analyzer",
                    "version": "1.0.0"
                }
            },
            "id": 1
        }
        
        response = requests.post(self.server_url, json=init_data, timeout=10)
        if response.status_code != 200:
            print(f"   åˆå§‹åŒ–å¤±è´¥: {response.status_code}")
            return False
        
        # è·å–ä¼šè¯ID
        self.session_id = response.headers.get('Mcp-Session-Id')
        if self.session_id:
            print(f"   ä¼šè¯ID: {self.session_id}")
            self.headers = {
                "Content-Type": "application/json",
                "Mcp-Session-Id": self.session_id
            }
            return True
        else:
            print("   æœªæ‰¾åˆ°ä¼šè¯ID")
            return False
    
    def call_tool(self, tool_name, arguments):
        """è°ƒç”¨MCPå·¥å…·"""
        call_data = {
            "jsonrpc": "2.0",
            "method": "tools/call",
            "params": {
                "name": tool_name,
                "arguments": arguments
            },
            "id": int(time.time() * 1000) % 10000
        }
        
        try:
            response = requests.post(
                self.server_url, 
                headers=self.headers, 
                json=call_data, 
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                if 'error' in result:
                    print(f"   å·¥å…·è°ƒç”¨é”™è¯¯: {result['error']}")
                    return None
                elif 'result' in result:
                    return result['result']
            else:
                print(f"   HTTPé”™è¯¯: {response.status_code}")
                return None
                
        except Exception as e:
            print(f"   è°ƒç”¨å¼‚å¸¸: {e}")
            return None
    
    def search_recent_feeds(self, keyword="çˆ†æ¬¾", days=15):
        """æœç´¢è¿‘æœŸçš„å†…å®¹"""
        print(f"\nğŸ” æœç´¢è¿‘{days}å¤©çš„å†…å®¹...")
        
        # å°è¯•ä¸åŒçš„æœç´¢å‚æ•°
        search_params = [
            {"keyword": keyword},
            {"keyword": "çƒ­é—¨"},
            {"keyword": "æ¨è"}
        ]
        
        all_feeds = []
        
        for params in search_params:
            print(f"   æœç´¢å…³é”®è¯: {params['keyword']}")
            result = self.call_tool("search_feeds", params)
            
            if result and 'content' in result:
                content = result['content']
                if content and isinstance(content, list):
                    for item in content:
                        if 'text' in item:
                            try:
                                feeds_data = json.loads(item['text'])
                                if 'feeds' in feeds_data:
                                    all_feeds.extend(feeds_data['feeds'])
                            except:
                                pass
            
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        # å¦‚æœæ²¡æœ‰æœç´¢ç»“æœï¼Œå°è¯•ä½¿ç”¨list_feeds
        if not all_feeds:
            print("   ä½¿ç”¨list_feedsè·å–å†…å®¹...")
            result = self.call_tool("list_feeds", {})
            if result and 'content' in result:
                content = result['content']
                if content and isinstance(content, list):
                    for item in content:
                        if 'text' in item:
                            try:
                                feeds_data = json.loads(item['text'])
                                if 'feeds' in feeds_data:
                                    all_feeds.extend(feeds_data['feeds'])
                            except:
                                pass
        
        # è¿‡æ»¤è¿‘æœŸçš„å†…å®¹
        recent_feeds = []
        cutoff_date = datetime.now() - timedelta(days=days)
        
        for feed in all_feeds:
            # è¿™é‡Œå¯ä»¥æ ¹æ®å®é™…æ•°æ®ç»“æ„è°ƒæ•´æ—¶é—´è¿‡æ»¤é€»è¾‘
            # ç”±äºAPIå¯èƒ½ä¸è¿”å›æ—¶é—´ï¼Œæˆ‘ä»¬å…ˆä¿ç•™æ‰€æœ‰å†…å®¹
            recent_feeds.append(feed)
        
        print(f"   æ‰¾åˆ° {len(recent_feeds)} æ¡å†…å®¹")
        return recent_feeds
    
    def extract_keywords(self, feeds, top_n=20):
        """ä»å†…å®¹ä¸­æå–å…³é”®è¯"""
        print(f"\nğŸ“Š æå–å…³é”®è¯ (å‰{top_n}ä¸ª)...")
        
        # æ”¶é›†æ‰€æœ‰æ–‡æœ¬
        all_text = ""
        titles = []
        
        for feed in feeds:
            if 'noteCard' in feed:
                note_card = feed['noteCard']
                if 'displayTitle' in note_card:
                    title = note_card['displayTitle']
                    titles.append(title)
                    all_text += title + " "
                
                # å¦‚æœæœ‰äº’åŠ¨ä¿¡æ¯ï¼Œå¯ä»¥è®°å½•ç‚¹èµæ•°
                if 'interactInfo' in note_card:
                    interact = note_card['interactInfo']
                    likes = interact.get('likedCount', '0')
                    # å¯ä»¥ç”¨äºç­›é€‰çˆ†æ¬¾å†…å®¹
        
        if not all_text:
            return []
        
        # ä½¿ç”¨jiebaæå–å…³é”®è¯
        jieba.analyse.set_stop_words("stop_words.txt")  # å¦‚æœæœ‰åœç”¨è¯æ–‡ä»¶
        
        # æå–å…³é”®è¯
        keywords = jieba.analyse.extract_tags(
            all_text, 
            topK=top_n, 
            withWeight=True,
            allowPOS=('n', 'vn', 'v', 'a', 'nr', 'ns', 'nt', 'nz')
        )
        
        return keywords, titles
    
    def analyze_trends(self, feeds):
        """åˆ†æè¶‹åŠ¿å’Œæ¨¡å¼"""
        print("\nğŸ“ˆ åˆ†æè¶‹åŠ¿æ¨¡å¼...")
        
        trends = {
            "çƒ­é—¨è¯é¢˜": [],
            "é«˜é¢‘è¯æ±‡": [],
            "å†…å®¹ç±»å‹": {},
            "æƒ…æ„Ÿå€¾å‘": {"positive": 0, "negative": 0, "neutral": 0}
        }
        
        # åˆ†æå†…å®¹ç±»å‹
        content_types = Counter()
        for feed in feeds:
            if 'noteCard' in feed:
                note_card = feed['noteCard']
                title = note_card.get('displayTitle', '')
                
                # ç®€å•åˆ†ç±»
                if any(word in title for word in ['ç¾é£Ÿ', 'åƒ', 'é¤å…', 'æ–™ç†']):
                    content_types['ç¾é£Ÿ'] += 1
                elif any(word in title for word in ['ç©¿æ­', 'è¡£æœ', 'æ—¶å°š', 'æ­é…']):
                    content_types['æ—¶å°šç©¿æ­'] += 1
                elif any(word in title for word in ['æ—…æ¸¸', 'æ—…è¡Œ', 'æ™¯ç‚¹', 'æ‰“å¡']):
                    content_types['æ—…æ¸¸'] += 1
                elif any(word in title for word in ['ç¾å¦†', 'åŒ–å¦†', 'æŠ¤è‚¤', 'ç¾å®¹']):
                    content_types['ç¾å¦†æŠ¤è‚¤'] += 1
                elif any(word in title for word in ['ç”Ÿæ´»', 'æ—¥å¸¸', 'vlog', 'è®°å½•']):
                    content_types['ç”Ÿæ´»æ—¥å¸¸'] += 1
                elif any(word in title for word in ['å­¦ä¹ ', 'çŸ¥è¯†', 'å¹²è´§', 'æ•™ç¨‹']):
                    content_types['çŸ¥è¯†å¹²è´§'] += 1
                elif any(word in title for word in ['æƒ…æ„Ÿ', 'æ‹çˆ±', 'å©šå§»', 'æ„Ÿæƒ…']):
                    content_types['æƒ…æ„Ÿ'] += 1
                elif any(word in title for word in ['æç¬‘', 'å¹½é»˜', 'æ®µå­', 'ç¬‘è¯']):
                    content_types['æç¬‘å¨±ä¹'] += 1
                else:
                    content_types['å…¶ä»–'] += 1
        
        trends["å†…å®¹ç±»å‹"] = dict(content_types)
        
        return trends
    
    def generate_report(self, keywords, titles, trends, days=15):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print("\nğŸ“‹ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        
        report = f"""# å°çº¢ä¹¦è¿‘{days}å¤©çˆ†æ¬¾æ–‡æ¡ˆåˆ†ææŠ¥å‘Š
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
åˆ†æå†…å®¹æ•°é‡: {len(titles)}

## ğŸ“Š å…³é”®è¯åˆ†æ
"""
        
        # å…³é”®è¯éƒ¨åˆ†
        report += "| æ’å | å…³é”®è¯ | æƒé‡ |\n"
        report += "|------|--------|------|\n"
        for i, (keyword, weight) in enumerate(keywords, 1):
            report += f"| {i} | {keyword} | {weight:.4f} |\n"
        
        # è¶‹åŠ¿åˆ†æ
        report += f"\n## ğŸ“ˆ å†…å®¹ç±»å‹åˆ†å¸ƒ\n"
        for content_type, count in trends["å†…å®¹ç±»å‹"].items():
            percentage = (count / len(titles)) * 100 if titles else 0
            report += f"- **{content_type}**: {count} æ¡ ({percentage:.1f}%)\n"
        
        # çƒ­é—¨æ ‡é¢˜ç¤ºä¾‹
        report += f"\n## ğŸ”¥ çƒ­é—¨æ ‡é¢˜ç¤ºä¾‹\n"
        for i, title in enumerate(titles[:10], 1):
            report += f"{i}. {title}\n"
        
        # çˆ†æ¬¾æ–‡æ¡ˆç‰¹å¾æ€»ç»“
        report += f"""
## ğŸ’¡ çˆ†æ¬¾æ–‡æ¡ˆç‰¹å¾æ€»ç»“

### 1. æ ‡é¢˜ç‰¹å¾
- **æ•°å­—å¸å¼•**: ä½¿ç”¨å…·ä½“æ•°å­—å¢åŠ å¯ä¿¡åº¦
- **æƒ…ç»ªè¯**: ä½¿ç”¨æ„Ÿå¹è¯ã€è¡¨æƒ…ç¬¦å·å¢å¼ºæƒ…æ„Ÿè¡¨è¾¾
- **ç–‘é—®å¥å¼**: å¼•å‘è¯»è€…å¥½å¥‡å’Œäº’åŠ¨
- **åˆ©ç›Šç‚¹æ˜ç¡®**: ç›´æ¥è¯´æ˜èƒ½ç»™è¯»è€…å¸¦æ¥çš„ä»·å€¼

### 2. å†…å®¹ç»“æ„
- **å¼€å¤´æŠ“çœ¼çƒ**: å‰3ç§’å†³å®šç”¨æˆ·æ˜¯å¦ç»§ç»­é˜…è¯»
- **ä¸­é—´æœ‰ä»·å€¼**: æä¾›å®ç”¨ä¿¡æ¯æˆ–æƒ…æ„Ÿå…±é¸£
- **ç»“å°¾æœ‰è¡ŒåŠ¨**: å¼•å¯¼ç‚¹èµã€æ”¶è—ã€è¯„è®ºæˆ–å…³æ³¨

### 3. çƒ­é—¨è¯é¢˜æ–¹å‘
"""
        
        # æ ¹æ®å…³é”®è¯æ¨èè¯é¢˜æ–¹å‘
        hot_keywords = [k for k, _ in keywords[:5]]
        report += f"- åŸºäºé«˜é¢‘å…³é”®è¯ {', '.join(hot_keywords)} çš„å†…å®¹æ›´å®¹æ˜“è·å¾—å…³æ³¨\n"
        
        # åˆ›ä½œå»ºè®®
        report += f"""
## ğŸ¯ åˆ›ä½œå»ºè®®

1. **ç»“åˆçƒ­ç‚¹**: å…³æ³¨å½“å‰çƒ­é—¨è¯é¢˜å’ŒèŠ‚æ—¥èŠ‚ç‚¹
2. **çªå‡ºä»·å€¼**: æ ‡é¢˜æ˜ç¡®è¯´æ˜å†…å®¹èƒ½ç»™è¯»è€…å¸¦æ¥çš„å¥½å¤„
3. **æƒ…æ„Ÿå…±é¸£**: ä½¿ç”¨èƒ½å¼•å‘æƒ…æ„Ÿå…±é¸£çš„è¯­è¨€
4. **è§†è§‰å¸å¼•**: é…åˆé«˜è´¨é‡çš„å›¾ç‰‡æˆ–è§†é¢‘
5. **äº’åŠ¨å¼•å¯¼**: æ˜ç¡®å¼•å¯¼ç”¨æˆ·ç‚¹èµã€è¯„è®ºã€æ”¶è—

## ğŸ“ ç¤ºä¾‹çˆ†æ¬¾æ ‡é¢˜æ¨¡æ¿

1. "æˆ‘å®£å¸ƒXXXæ˜¯ä»Šå¹´æœ€XXXçš„XXXï¼"
2. "XXXä¸ªXXXæŠ€å·§ï¼Œè®©ä½ XXXä¸å†XXX"
3. "XXXåŸæ¥è¦è¿™æ ·XXXï¼åæ‚”æ²¡æ—©ç‚¹çŸ¥é“"
4. "XXXçš„XXXï¼ŒXXXäººéƒ½è¯´XXX"
5. "XXX vs XXXï¼Œå“ªä¸ªæ›´XXXï¼Ÿ"

---
*åˆ†æåŸºäºå°çº¢ä¹¦MCPæœåŠ¡å™¨è·å–çš„å…¬å¼€å†…å®¹ï¼Œä»…ä¾›å‚è€ƒ*
"""
        
        return report
    
    def run_analysis(self, days=15):
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        print("="*60)
        print("ğŸ“± å°çº¢ä¹¦çˆ†æ¬¾æ–‡æ¡ˆåˆ†æç³»ç»Ÿ")
        print("="*60)
        
        # åˆ›å»ºä¼šè¯
        if not self.create_session():
            print("âŒ æ— æ³•åˆ›å»ºMCPä¼šè¯")
            return
        
        # æœç´¢è¿‘æœŸå†…å®¹
        feeds = self.search_recent_feeds(days=days)
        
        if not feeds:
            print("âŒ æœªæ‰¾åˆ°å†…å®¹æ•°æ®")
            return
        
        # æå–å…³é”®è¯
        keywords, titles = self.extract_keywords(feeds)
        
        if not keywords:
            print("âŒ æ— æ³•æå–å…³é”®è¯")
            return
        
        # åˆ†æè¶‹åŠ¿
        trends = self.analyze_trends(feeds)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = self.generate_report(keywords, titles, trends, days)
        
        # ä¿å­˜æŠ¥å‘Š
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"å°çº¢ä¹¦çˆ†æ¬¾æ–‡æ¡ˆåˆ†ææŠ¥å‘Š_{timestamp}.md"
        
        with open(report_file, "w", encoding="utf-8") as f:
            f.write(report)
        
        print(f"\nâœ… åˆ†æå®Œæˆ!")
        print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        print("\n" + "="*60)
        
        # æ‰“å°æ‘˜è¦
        print("\nğŸ“‹ æŠ¥å‘Šæ‘˜è¦:")
        print("-" * 40)
        lines = report.split('\n')
        for line in lines[:50]:  # æ‰“å°å‰50è¡Œä½œä¸ºæ‘˜è¦
            print(line)
        
        return report

def main():
    """ä¸»å‡½æ•°"""
    analyzer = XiaohongshuAnalyzer()
    
    try:
        report = analyzer.run_analysis(days=15)
        
        if report:
            # ä¿å­˜ç®€ç‰ˆæŠ¥å‘Š
            with open("å°çº¢ä¹¦çˆ†æ¬¾å…³é”®è¯æ‘˜è¦.txt", "w", encoding="utf-8") as f:
                # æå–å…³é”®è¯éƒ¨åˆ†
                lines = report.split('\n')
                for line in lines:
                    if "| æ’å |" in line or line.startswith("| ") and "|" in line:
                        f.write(line + "\n")
                    
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()