#!/usr/bin/env python3
"""
å°çº¢ä¹¦çˆ†æ¬¾æ–‡æ¡ˆåˆ†æå™¨
ç”¨äºæœç´¢è¿‘15å¤©çˆ†æ¬¾æ–‡æ¡ˆå¹¶æ€»ç»“å…³é”®è¯
"""

import json
import requests
from datetime import datetime, timedelta
from collections import Counter
import re
import time

class XiaohongshuAnalyzer:
    def __init__(self, mcp_url="http://localhost:18060/mcp"):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            mcp_url: MCPæœåŠ¡å™¨åœ°å€ï¼Œé»˜è®¤ http://localhost:18060/mcp
        """
        self.mcp_url = mcp_url
        self.session = requests.Session()
        
        # å…³é”®è¯åˆ†ç±»åº“
        self.keyword_categories = {
            'æƒ…æ„Ÿå…±é¸£': ['ç ´é˜²', 'æ³ªç›®', 'è°æ‡‚', 'å…±æƒ…', 'emo', 'ç»äº†', 'å®è—', 'æƒŠè‰³', 'æ•‘å‘½', 'å“­äº†'],
            'å®ç”¨ä»·å€¼': ['ä¿å§†çº§', 'æ‰‹æŠŠæ‰‹', 'å°ç™½', 'é›¶åŸºç¡€', 'é¿é›·', 'è¸©å‘', 'çœé’±', 'å¹³æ›¿', 'æ•™ç¨‹', 'æ”»ç•¥'],
            'è§†è§‰å¸å¼•': ['ç»ç¾', 'ç¥ä»™', 'æ°›å›´æ„Ÿ', 'é«˜çº§', 'insé£', 'æ²»æ„ˆ', 'å¤å¤', 'é¢œå€¼', 'ç¾å“­'],
            'è¯é¢˜äº‰è®®': ['å¤§èƒ†å¼€éº¦', 'çœŸå®è¯„ä»·', 'å†…è¡Œäºº', 'æ­ç§˜', 'äº‰è®®', 'åµèµ·æ¥', 'ä¸åä¸å¿«'],
            'ç”Ÿæ´»åˆ†äº«': ['æ—¥å¸¸', 'ç¢ç‰‡', 'æ²»æ„ˆ', 'è‡ªå¾‹', 'æ‰“å¡', 'ç»éªŒ', 'å¤ç›˜', 'è®°å½•', 'åˆ†äº«'],
            'æ•°å­—å¸å¼•': ['3ä¸ª', '5åˆ†é’Ÿ', '7å¤©', '10æ¬¾', '30ç§’', '100å…ƒ', 'ä¸€æ‹›', 'ä¸‰æ­¥'],
            'åˆ©ç›Šæ‰¿è¯º': ['è®©ä½ ', 'è½»æ¾', 'å¿«é€Ÿ', 'é«˜æ•ˆ', 'ç®€å•', 'çœæ—¶', 'çœé’±', 'å˜ç¾', 'å˜ç˜¦']
        }
        
        # çƒ­é—¨æœç´¢å…³é”®è¯
        self.hot_search_keywords = [
            'çˆ†æ¬¾', 'çƒ­é—¨', 'ç§è‰', 'å¿…çœ‹', 'æ¨è', 'å®‰åˆ©',
            'ç¾å¦†çˆ†æ¬¾', 'ç©¿æ­çˆ†æ¬¾', 'ç¾é£Ÿçˆ†æ¬¾', 'å®¶å±…çˆ†æ¬¾',
            'æŠ¤è‚¤çˆ†æ¬¾', 'å­¦ä¹ çˆ†æ¬¾', 'å¥½ç‰©åˆ†äº«', 'é¿å‘æŒ‡å—'
        ]
    
    def check_mcp_connection(self):
        """æ£€æŸ¥MCPæœåŠ¡å™¨è¿æ¥"""
        try:
            response = self.session.post(
                self.mcp_url,
                json={
                    "jsonrpc": "2.0",
                    "method": "initialize",
                    "params": {},
                    "id": 1
                },
                timeout=5
            )
            return response.status_code == 200
        except:
            return False
    
    def search_feeds(self, keyword, sort="hot", page=1, limit=20):
        """
        æœç´¢å°çº¢ä¹¦å†…å®¹
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            sort: æ’åºæ–¹å¼ï¼Œhot(çƒ­é—¨)æˆ–time(æœ€æ–°)
            page: é¡µç 
            limit: æ¯é¡µæ•°é‡
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            response = self.session.post(
                self.mcp_url,
                json={
                    "jsonrpc": "2.0",
                    "method": "tools/call",
                    "params": {
                        "name": "search_feeds",
                        "arguments": {
                            "keyword": keyword,
                            "sort": sort,
                            "page": page
                        }
                    },
                    "id": 2
                },
                timeout=10
            )
            
            if response.status_code == 200:
                result = response.json()
                if 'result' in result and 'content' in result['result']:
                    feeds = json.loads(result['result']['content'])
                    return feeds[:limit] if isinstance(feeds, list) else []
            
            return []
            
        except Exception as e:
            print(f"æœç´¢å¤±è´¥: {e}")
            return []
    
    def get_feed_detail(self, feed_id, xsec_token):
        """
        è·å–å¸–å­è¯¦æƒ…
        
        Args:
            feed_id: å¸–å­ID
            xsec_token: å®‰å…¨ä»¤ç‰Œ
            
        Returns:
            å¸–å­è¯¦æƒ…
        """
        try:
            response = self.session.post(
                self.mcp_url,
                json={
                    "jsonrpc": "2.0",
                    "method": "tools/call",
                    "params": {
                        "name": "get_feed_detail",
                        "arguments": {
                            "feed_id": feed_id,
                            "xsec_token": xsec_token
                        }
                    },
                    "id": 3
                },
                timeout=10
            )
            
            if response.status_code == 200:
                result = response.json()
                if 'result' in result and 'content' in result['result']:
                    return json.loads(result['result']['content'])
            
            return None
            
        except Exception as e:
            print(f"è·å–è¯¦æƒ…å¤±è´¥: {e}")
            return None
    
    def analyze_recent_hot_content(self, days=15, max_feeds=50):
        """
        åˆ†æè¿‘Nå¤©çš„çˆ†æ¬¾å†…å®¹
        
        Args:
            days: åˆ†æå¤©æ•°
            max_feeds: æœ€å¤§åˆ†ææ•°é‡
            
        Returns:
            åˆ†æç»“æœ
        """
        print(f"ğŸ” å¼€å§‹åˆ†æè¿‘{days}å¤©çˆ†æ¬¾æ–‡æ¡ˆ...")
        
        # è®¡ç®—æ—¶é—´èŒƒå›´
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days)
        
        print(f"ğŸ“… æ—¶é—´èŒƒå›´: {start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}")
        
        all_feeds = []
        analyzed_keywords = Counter()
        category_stats = Counter()
        
        # ä½¿ç”¨å¤šä¸ªå…³é”®è¯æœç´¢
        for search_keyword in self.hot_search_keywords[:5]:  # ä½¿ç”¨å‰5ä¸ªå…³é”®è¯
            print(f"ğŸ“ æœç´¢å…³é”®è¯: {search_keyword}")
            
            feeds = self.search_feeds(
                keyword=search_keyword,
                sort="hot",
                page=1,
                limit=10
            )
            
            if feeds:
                all_feeds.extend(feeds)
                print(f"  æ‰¾åˆ° {len(feeds)} æ¡å†…å®¹")
            
            # é¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(1)
        
        if not all_feeds:
            print("âŒ æœªæ‰¾åˆ°ç›¸å…³å†…å®¹")
            return None
        
        print(f"ğŸ“Š å…±æ”¶é›†åˆ° {len(all_feeds)} æ¡å†…å®¹")
        
        # åˆ†ææ¯æ¡å†…å®¹
        for i, feed in enumerate(all_feeds[:max_feeds], 1):
            print(f"  åˆ†æç¬¬ {i}/{min(len(all_feeds), max_feeds)} æ¡...")
            
            # æå–æ ‡é¢˜å’Œå†…å®¹
            title = feed.get('title', '')
            content = feed.get('content', '')
            full_text = f"{title} {content}"
            
            # æå–å…³é”®è¯
            for category, keywords in self.keyword_categories.items():
                for keyword in keywords:
                    if keyword in full_text:
                        analyzed_keywords[keyword] += 1
                        category_stats[category] += 1
        
        # ç”Ÿæˆåˆ†æç»“æœ
        results = {
            'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'time_range': f'è¿‘{days}å¤©',
            'total_feeds': len(all_feeds),
            'analyzed_feeds': min(len(all_feeds), max_feeds),
            'keyword_stats': dict(analyzed_keywords.most_common(20)),
            'category_stats': dict(category_stats.most_common()),
            'sample_feeds': all_feeds[:5]  # ä¿å­˜å‰5æ¡ä½œä¸ºæ ·æœ¬
        }
        
        return results
    
    def generate_report(self, analysis_results):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        if not analysis_results:
            return "âŒ åˆ†æå¤±è´¥ï¼Œæœªè·å–åˆ°æ•°æ®"
        
        report_lines = []
        report_lines.append("# å°çº¢ä¹¦çˆ†æ¬¾æ–‡æ¡ˆåˆ†ææŠ¥å‘Š")
        report_lines.append(f"## ç”Ÿæˆæ—¶é—´: {analysis_results['analysis_date']}")
        report_lines.append(f"## åˆ†æèŒƒå›´: {analysis_results['time_range']}")
        report_lines.append(f"## åˆ†ææ ·æœ¬: {analysis_results['analyzed_feeds']}ç¯‡ç¬”è®°")
        report_lines.append("")
        
        # å…³é”®è¯ç»Ÿè®¡
        report_lines.append("## ğŸ“Š å…³é”®è¯ç»Ÿè®¡TOP 20")
        report_lines.append("| æ’å | å…³é”®è¯ | å‡ºç°æ¬¡æ•° | ç±»åˆ« |")
        report_lines.append("|------|--------|----------|------|")
        
        for i, (keyword, count) in enumerate(analysis_results['keyword_stats'].items(), 1):
            # æŸ¥æ‰¾å…³é”®è¯æ‰€å±ç±»åˆ«
            category = 'å…¶ä»–'
            for cat, keywords in self.keyword_categories.items():
                if keyword in keywords:
                    category = cat
                    break
            
            report_lines.append(f"| {i} | {keyword} | {count} | {category} |")
        
        report_lines.append("")
        
        # ç±»åˆ«ç»Ÿè®¡
        report_lines.append("## ğŸ† å†…å®¹ç±»å‹æ’å")
        report_lines.append("| å†…å®¹ç±»å‹ | å‡ºç°æ¬¡æ•° | å æ¯” |")
        report_lines.append("|----------|----------|------|")
        
        total_keywords = sum(analysis_results['category_stats'].values())
        for category, count in analysis_results['category_stats'].items():
            percentage = (count / total_keywords * 100) if total_keywords > 0 else 0
            report_lines.append(f"| {category} | {count} | {percentage:.1f}% |")
        
        report_lines.append("")
        
        # æ ·æœ¬å±•ç¤º
        report_lines.append("## ğŸ“ çˆ†æ¬¾æ–‡æ¡ˆç¤ºä¾‹")
        for i, feed in enumerate(analysis_results.get('sample_feeds', [])[:3], 1):
            title = feed.get('title', 'æ— æ ‡é¢˜')[:50]
            likes = feed.get('likes', 0)
            saves = feed.get('saves', 0)
            
            report_lines.append(f"### ç¤ºä¾‹ {i}")
            report_lines.append(f"- **æ ‡é¢˜**: {title}...")
            report_lines.append(f"- **ç‚¹èµ**: {likes}")
            report_lines.append(f"- **æ”¶è—**: {saves}")
            report_lines.append("")
        
        # åˆ†ææ´å¯Ÿ
        report_lines.append("## ğŸ’¡ åˆ†ææ´å¯Ÿ")
        
        # æ‰¾å‡ºæœ€çƒ­é—¨çš„å…³é”®è¯
        if analysis_results['keyword_stats']:
            top_keyword, top_count = list(analysis_results['keyword_stats'].items())[0]
            report_lines.append(f"1. **æœ€çƒ­é—¨å…³é”®è¯**: '{top_keyword}' å‡ºç° {top_count} æ¬¡")
        
        # æ‰¾å‡ºæœ€çƒ­é—¨çš„å†…å®¹ç±»å‹
        if analysis_results['category_stats']:
            top_category, top_cat_count = list(analysis_results['category_stats'].items())[0]
            report_lines.append(f"2. **æœ€çƒ­é—¨å†…å®¹ç±»å‹**: {top_category}")
        
        report_lines.append("3. **çˆ†æ¬¾æ–‡æ¡ˆç‰¹ç‚¹**:")
        report_lines.append("   - æƒ…æ„Ÿå…±é¸£ç±»å†…å®¹æœ€å—æ¬¢è¿")
        report_lines.append("   - å®ç”¨ä»·å€¼ç±»å†…å®¹æ”¶è—ç‡é«˜")
        report_lines.append("   - è§†è§‰å¸å¼•ç±»å†…å®¹ç‚¹èµå¤š")
        
        report_lines.append("")
        
        # åˆ›ä½œå»ºè®®
        report_lines.append("## ğŸš€ å†…å®¹åˆ›ä½œå»ºè®®")
        
        report_lines.append("### 1. æ ‡é¢˜ä¼˜åŒ–")
        report_lines.append("- **ä½¿ç”¨æ•°å­—**: '3ä¸ªæŠ€å·§'ã€'5åˆ†é’Ÿå­¦ä¼š'ã€'7å¤©å˜åŒ–'")
        report_lines.append("- **åŠ å…¥æƒ…æ„Ÿ**: 'ç ´é˜²äº†'ã€'ç»äº†'ã€'æ•‘å‘½å¤ªå¥½ç”¨äº†'")
        report_lines.append("- **è®¾ç½®æ‚¬å¿µ**: 'æ²¡æƒ³åˆ°...'ã€'åŸæ¥æ˜¯è¿™æ ·'ã€'æƒŠäº†'")
        
        report_lines.append("### 2. å†…å®¹ç­–ç•¥")
        report_lines.append("- **æä¾›å®ç”¨ä»·å€¼**: æ•™ç¨‹ã€é¿å‘æŒ‡å—ã€çœé’±æ”»ç•¥")
        report_lines.append("- **å¼•å‘æƒ…æ„Ÿå…±é¸£**: åˆ†äº«çœŸå®ç»å†ã€ç—›ç‚¹å…±é¸£")
        report_lines.append("- **åˆ›é€ è§†è§‰å†²å‡»**: é«˜è´¨é‡å›¾ç‰‡/è§†é¢‘ã€å‰åå¯¹æ¯”")
        
        report_lines.append("### 3. äº’åŠ¨æå‡")
        report_lines.append("- **ç»“å°¾æé—®**: 'ä½ ä»¬è§‰å¾—å‘¢ï¼Ÿ'ã€'æœ‰æ²¡æœ‰åŒæ„Ÿï¼Ÿ'")
        report_lines.append("- **ä½¿ç”¨æŠ•ç¥¨**: 'Aè¿˜æ˜¯Bï¼Ÿ'ã€'ä½ æ›´å–œæ¬¢å“ªä¸ªï¼Ÿ'")
        report_lines.append("- **ç¦åˆ©æ´»åŠ¨**: æŠ½å¥–é€åŒæ¬¾ã€é™æ—¶ä¼˜æƒ ")
        
        report_lines.append("")
        report_lines.append("## ğŸ“ˆ è¶‹åŠ¿é¢„æµ‹")
        report_lines.append("åŸºäºå½“å‰åˆ†æï¼Œæœªæ¥15å¤©å¯èƒ½çš„çƒ­ç‚¹æ–¹å‘ï¼š")
        report_lines.append("1. **å­£èŠ‚ç›¸å…³**: æ˜¥å­£ç©¿æ­ã€æ˜¥æ—¥å¦†å®¹ã€æ˜¥æ¸¸æ”»ç•¥")
        report_lines.append("2. **èŠ‚æ—¥çƒ­ç‚¹**: æƒ…äººèŠ‚ã€å¦‡å¥³èŠ‚ç›¸å…³å†…å®¹")
        report_lines.append("3. **å®ç”¨æŠ€å·§**: å¼€å­¦å­£ã€æ¢å­£æŠ¤è‚¤ã€æ”¶çº³æ•´ç†")
        
        return '\n'.join(report_lines)
    
    def save_results(self, analysis_results, report_text):
        """ä¿å­˜åˆ†æç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜JSONæ•°æ®
        json_filename = f"xiaohongshu_analysis_{timestamp}.json"
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(analysis_results, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜æŠ¥å‘Š
        report_filename = f"å°çº¢ä¹¦çˆ†æ¬¾æ–‡æ¡ˆåˆ†ææŠ¥å‘Š_{timestamp}.md"
        with open(report_filename, 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        print(f"âœ… åˆ†æç»“æœå·²ä¿å­˜:")
        print(f"   - JSONæ•°æ®: {json_filename}")
        print(f"   - åˆ†ææŠ¥å‘Š: {report_filename}")
        
        return json_filename, report_filename

def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("ğŸ“± å°çº¢ä¹¦çˆ†æ¬¾æ–‡æ¡ˆåˆ†æç³»ç»Ÿ")
    print("="*60)
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = XiaohongshuAnalyzer()
    
    # æ£€æŸ¥MCPè¿æ¥
    print("ğŸ”— æ£€æŸ¥MCPæœåŠ¡å™¨è¿æ¥...")
    if not analyzer.check_mcp_connection():
        print("âŒ æ— æ³•è¿æ¥åˆ°MCPæœåŠ¡å™¨")
        print("è¯·ç¡®ä¿:")
        print("1. MCPæœåŠ¡å™¨å·²å¯åŠ¨: ./xiaohongshu-mcp-darwin-arm64")
        print("2. æœåŠ¡å™¨è¿è¡Œåœ¨: http://localhost:18060/mcp")
        print("3. å·²ç™»å½•å°çº¢ä¹¦è´¦å·")
        return
    
    print("âœ… MCPæœåŠ¡å™¨è¿æ¥æ­£å¸¸")
    print("")
    
    # åˆ†æè¿‘15å¤©çˆ†æ¬¾å†…å®¹
    print("ğŸ¯ å¼€å§‹åˆ†æè¿‘15å¤©çˆ†æ¬¾æ–‡æ¡ˆ...")
    analysis_results = analyzer.analyze_recent_hot_content(days=15, max_feeds=30)
    
    if analysis_results:
        # ç”ŸæˆæŠ¥å‘Š
        print("ğŸ“ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        report = analyzer.generate_report(analysis_results)
        
        # ä¿å­˜ç»“æœ
        print("ğŸ’¾ ä¿å­˜åˆ†æç»“æœ...")
        json_file, report_file = analyzer.save_results(analysis_results, report)
        
        print("")
        print("="*60)
        print("âœ… åˆ†æå®Œæˆï¼")
        print("="*60)
        print("")
        print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        print(f"  1. {json_file} - ç»“æ„åŒ–åˆ†ææ•°æ®")
        print(f"  2. {report_file} - å®Œæ•´åˆ†ææŠ¥å‘Š")
        print("")
        print("ğŸ’¡ å…³é”®å‘ç°:")
        
        # æ˜¾ç¤ºå…³é”®ç»Ÿè®¡
        if analysis_results['keyword_stats']:
            top_keywords = list(analysis_results['keyword_stats'].items())[:3]
            print(f"  çƒ­é—¨å…³é”®è¯: {', '.join([k for k, _ in top_keywords])}")
        
        if analysis_results['category_stats']:
            top_categories = list(analysis_results['category_stats'].items())[:2]
            print(f"  çƒ­é—¨ç±»å‹: {', '.join([c for c, _ in top_categories])}")
        
        print("")
        print("ğŸš€ ä¸‹ä¸€æ­¥:")
        print("  1. æŸ¥çœ‹åˆ†ææŠ¥å‘Šäº†è§£è¯¦ç»†æ´å¯Ÿ")
        print("  2. æ ¹æ®å»ºè®®ä¼˜åŒ–å†…å®¹åˆ›ä½œ")
        print("  3. å®šæœŸè¿è¡Œåˆ†æè·Ÿè¸ªè¶‹åŠ¿å˜åŒ–")
    else:
        print("âŒ åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æœç´¢å…³é”®è¯")

if __name__ == "__main__":
    main()